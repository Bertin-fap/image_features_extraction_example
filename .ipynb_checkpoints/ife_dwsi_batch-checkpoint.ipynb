{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing of the confocal images\n",
    "### Inputs \n",
    "Confocal images as .plu files (binary standard of Sensofar company)\n",
    "#### Input files organization \n",
    "For several images 'wafer #-&' per wafer 'wafer #' per cut 'cut ###'\n",
    "<br>\n",
    "the files organization is as follows:\n",
    "<br>\n",
    "root / cut ### / wafer # / save_plu / wafer #-&.plu\n",
    "\n",
    "<br>\n",
    "where:\n",
    "\n",
    "- & stands for a, b, c, d, e \n",
    "- \\# stands for a digit\n",
    "- total number of files is open for each 'wafer #'\n",
    "- total number of cuts ### defined by length of list cuts\n",
    "\n",
    "### Outputs\n",
    "5 files per image 'wafer #-&' \n",
    "- EXCEL file 'wafer#-&.xlsx' containing the morphological information of the features\n",
    "- JPEG file 'wafer#-&-im-raw.jpg' of the corrected raw image with indication of the corrected pixels\n",
    "- JPEG file 'wafer#-&-im-rebuild.jpg' of the features represented in colored circular dots of equivalent area as the real features\n",
    "with color scale relative to the features depth\n",
    "- JPEG file 'wafer#-&-im-bbox.jpg' of the features size histogram\n",
    "- JPEG file 'wafer#-&-im-hist.jpg' of box plot of the features size\n",
    "\n",
    "A $\\LaTeX$ file 'cut_report ###.tex' as a report gathering the 4 JPEG files and a graph per cut integrating all the corresponding box plots \n",
    "\n",
    "#### output files location \n",
    "'root / cut ### / wafer # / results /' for 'wafer#-&.xlsx' and the JPEG files \n",
    "<br>\n",
    "'root / cut ### /' for cut_report ###.tex\n",
    "<br>\n",
    "where:\n",
    "- & stands for a, b, c, d, e \n",
    "- \\# stands for a digit, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "# Path of 'site-packages' where useful packages are stored on MAC-OS\n",
    "mac_packages = \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages\"   \n",
    "\n",
    "# Specification of the directories of the experimental files\n",
    "root = Path(\"C:/Users/franc/OneDrive/Bureau/confocal/fichiers\") \n",
    "if not os.path.isdir(root) : \n",
    "    root = Path('/Users/amal/Gwyddion Images /Carton-Louise')\n",
    "cuts = [\"cut 117\",\"cut 119\",\"cut 121\",\"cut 134\",\"cut 139\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd party imports\n",
    "# Add package image_features_extract\n",
    "try: # standard storage path of 'site-packages' on WIN\n",
    "    import image_features_extract as ife\n",
    "except: # Add storage path of 'site-packages' on MAC-OS\n",
    "    import sys\n",
    "    sys.path.append(mac_packages)\n",
    "    import image_features_extract as ife\n",
    "\n",
    "# Function to find the parent directory of the path x\n",
    "parent_dir = lambda x: os.path.split(x)[0] if os.path.isdir(x) \\\n",
    "             else os.path.split(os.path.dirname(x))[0]\n",
    "\n",
    "# parameters initialization\n",
    "param = {}\n",
    "param['Top_hat_length'] = 50  #  should be larger than the largest feature to extract\n",
    "param['threshold'] = -0.3     # should be larger than the residual image background\n",
    "param['root'] = root\n",
    "\n",
    "\n",
    "for cut in cuts: # process the cuts\n",
    "    nbr_wafer = len([x for x in os.listdir(root / Path(cut)) if x[0:5]=='wafer'])\n",
    "    REP = [root / Path(cut) /Path(\"wafer \"+str(i))/ Path('save_plu') for i in range(1,nbr_wafer + 1)]\n",
    "    param['cut'] = cut\n",
    "    nbr_rep = len(REP)\n",
    "    mode = 'header'                         # mode \"header\" open a new rapport and flush an header\n",
    "    \n",
    "    for idx_rep,rep in enumerate(REP): # process the wafers\n",
    "        param['repertoire'] = rep\n",
    "        param['dir_results'] = parent_dir(param['repertoire'])/Path('results')\n",
    "        param['dir_tex'] = root / Path(cut)  #parent_dir(param['repertoire'])\n",
    "        param['wafer'] = \"Wafer \" + str(idx_rep+1)\n",
    "        if not os.path.exists(param['dir_results']):\n",
    "            os.mkdir(param['dir_results'])\n",
    "        files = [y for y in os.listdir( param['repertoire']) if (y.split('.')[-1] == 'plu')]\n",
    "        nbr_file = len(files)\n",
    "        \n",
    "        for idx_file,file in enumerate(files): # process the files\n",
    "            param['file'] = file\n",
    "            print(cut,file,idx_file+1,nbr_file)\n",
    "            \n",
    "            # we process the image\n",
    "            iconfocal_img, im_corr, im_bin, df = ife.image_processing_1(param,analyse_morpho=True)\n",
    "            \n",
    "            # if we process the last image we add a tailer to the tech document\n",
    "            if (idx_file+1 == nbr_file) & (idx_rep+1 == nbr_rep) : mode = 'tailer'\n",
    "                \n",
    "            # we add stuff to the tex report    \n",
    "            ife.make_tex_document_1(param, df, mode)\n",
    "            mode = 'corpus' # append section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature morphological statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "# Specification of the full output file name \n",
    "root = Path(\"C:/Users/franc/OneDrive/Bureau/confocal/fichiers\")\n",
    "if not os.path.isdir(root) : \n",
    "    root = Path('/Users/amal/Gwyddion Images /Carton-Louise') \n",
    "store_file = root / Path('synthesis.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates an excel folder with sheets named after the cuts_stat\n",
    "\n",
    "Each sheet is organized as follow:\n",
    "\n",
    "        area# nb_dot# mean_area# area_small# nb_dot_small# mean_area_small# area_large# nb_dot_large# mean area_large# \n",
    "wafer &\n",
    "wafer &\n",
    "wafer &\n",
    "wafer &\n",
    "wafer &\n",
    "\n",
    "where:\n",
    "    & = a, b, c, d,... is the label of the wafer\n",
    "    # = 1,2,.. is the label of the confocal image\n",
    "    \n",
    "    area# : total area of the features \n",
    "    nb_dot# : total number of features\n",
    "    mean_area# : mean area of the features\n",
    "    \n",
    "    area_small# : total area of small features (size < size_min)\n",
    "    nb_dot_small# : total number of small features\n",
    "    mean_area_small# : mean area of the small features\n",
    "    \n",
    "    area_large# : total area of large features (size >= size_min)\n",
    "    nb_dot_large# : total number of large features\n",
    "    mean_area_large# : mean area of the large features\n",
    "    \n",
    "'''\n",
    "# Standard library imports \n",
    "import re\n",
    "\n",
    "# 3rd party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "cuts_stat = [\"cut 117\",\"cut 119\",\"cut 121\",\"cut 134\",\"cut 139\"] # directories to be analysed\n",
    "color = {\"Wafer 1\":'k',\"Wafer 2\":'r',\"Wafer 3\":'b',\"Wafer 4\":'c',\"Wafer 5\":'g'}\n",
    "size_min =50 # size threshold used to distribute in two classes the features according to their size  \n",
    "\n",
    "#####\n",
    "#   # Statistical analysis and excel file generation \n",
    "#####\n",
    "\n",
    "writer = pd.ExcelWriter(store_file, engine='openpyxl')\n",
    "\n",
    "# dict used to plot the indent size vs indent depth \n",
    "indent_size = {}   # { cut label: list of all feature size  }\n",
    "indent_height = {} # { cut label: list of all feature depth  }\n",
    "indent_color = {}  # { cut label: list of all color define by the dict color  }\n",
    "\n",
    "for cut in cuts_stat:\n",
    "    nbr_wafer = len([x for x in os.listdir(root / Path(cut)) if x[0:5]=='wafer'])\n",
    "    rep_ = [root / Path(cut) /Path(\"wafer \"+str(i))/ Path('results') for i in range(1,nbr_wafer + 1)]\n",
    "    indent_size[cut] = []\n",
    "    indent_height[cut] = []\n",
    "    indent_color[cut] = []\n",
    "    flag = True # first round we create a data frame else we merge to the dataframe\n",
    "    \n",
    "    for idx_rep,rep in enumerate(rep_):\n",
    "        dir_wafer =\"Wafer \" + str(idx_rep+1)\n",
    "        \n",
    "        # build the list of file name wafer#-&.xlsx or wafer#-&.xlsx where # is a digit, & is a letter\n",
    "        files = []\n",
    "        for file in os.listdir( rep):\n",
    "            sub_str = re.findall(\"wafer\\s?\\d{1,3}-\\w.xlsx\", file)\n",
    "            if len(sub_str) : files.append(sub_str[0] )\n",
    "\n",
    "        size, size_a, size_b = [],[],[] # size of the features, size of the small features, size of the large features\n",
    "        nbr, nbr_a, nbr_b = [],[],[] # number of  features, number of  small features, number of  large features\n",
    "        mean, mean_a, mean_b = [],[],[] # mean size of  features, mean size of  small features, mean size of  large features\n",
    "        res = {}  # dict containing the features statistics per cut\n",
    "        \n",
    "        for idx_file,file in enumerate(files):\n",
    "            df = pd.read_excel(rep / Path(file))\n",
    "            \n",
    "            # update of the dicts \n",
    "            indent_size[cut].extend(np.array(df['size'].to_list()))\n",
    "            indent_height[cut].extend(np.array(df['height'].to_list()))\n",
    "            indent_color[cut].extend(color[dir_wafer]*len(df) )\n",
    "            \n",
    "            # stat of features \n",
    "            size.append(df['size'].sum())\n",
    "            nbr.append(len(df))\n",
    "            mean.append(df['size'].mean())\n",
    "            \n",
    "            # stat of large features with size > size_min\n",
    "            df1 = df.query(\"size >= @size_min\")\n",
    "            size_a.append(df1['size'].sum())\n",
    "            nbr_a.append(len(df1))\n",
    "            mean_a.append(df1['size'].mean())\n",
    "            \n",
    "            # stat of small features with size < size_min\n",
    "            df1 = df.query(\"size < @size_min\")\n",
    "            size_b.append(df1['size'].sum())\n",
    "            nbr_b.append(len(df1))\n",
    "            mean_b.append(df1['size'].mean())\n",
    "        \n",
    "        # update the dic res \n",
    "        res['area'+str(idx_rep+1)] = size \n",
    "        res['nb_dot'+str(idx_rep+1)] = nbr\n",
    "        res['mean area'+str(idx_rep+1)] = mean\n",
    "        res['area_small'+str(idx_rep+1)] = size_b \n",
    "        res['nb_dot_small'+str(idx_rep+1)] = nbr_b\n",
    "        res['mean area_small'+str(idx_rep+1)] = mean_b\n",
    "        res['area_large'+str(idx_rep+1)] = size_a \n",
    "        res['nb_dot_large'+str(idx_rep+1)] = nbr_a\n",
    "        res['mean area_large'+str(idx_rep+1)] = mean_a\n",
    "        \n",
    "        for key, value in res.items():  # add mean value and std \n",
    "            res[key] = value+[np.mean(value), np.std(value)]\n",
    "            \n",
    "        # flush dict into a data frame \n",
    "        if flag: # first round we create a new data frame\n",
    "            flag = False\n",
    "            dg = pd.DataFrame.from_dict(res)\n",
    "            \n",
    "        else: # we merge to the existing data frame\n",
    "            dg = dg.merge(pd.DataFrame.from_dict(res), left_index=True, right_index=True)\n",
    "        \n",
    "    dg.index = [\"wafer# a\",\"wafer# b\",\"wafer# c\",\"wafer# d\",\"wafer# e\",\"mean\",\"std\"] \n",
    "    \n",
    "    # flush the data frame into an excel file\n",
    "    dg.to_excel(writer, sheet_name=cut)\n",
    "    writer.save()\n",
    "    writer.close()\n",
    "\n",
    "#####\n",
    "#   # plot of the results \n",
    "#####\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "for idx, cut in enumerate(cuts_stat):\n",
    "    plt.subplot(3,2,idx+1)\n",
    "    x = -np.array(indent_height[cut])\n",
    "    y = np.array(indent_size[cut])\n",
    "    c = indent_color[cut]\n",
    "\n",
    "    plt.scatter(x,y, s=50, c=c)\n",
    "    plt.xlim(0.2,3.5)\n",
    "    plt.ylim(0,2000)\n",
    "    plt.xlabel('depth (µm)')\n",
    "    plt.ylabel('size (px\\u00B2)')\n",
    "    plt.title(cut)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "cuts = ['cut 117', 'cut 119', 'cut 121', 'cut 134', 'cut 139']\n",
    "centroid = {}\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "for idx, cut in enumerate(cuts):\n",
    "    plt.subplot(3,2,idx+1)\n",
    "    Data = {'x': -np.array(indent_height[cut]),\n",
    "            'y': np.array(indent_size[cut])\n",
    "           }\n",
    "    df = pd.DataFrame(Data,columns=['x','y'])\n",
    "    kmeans = KMeans(n_clusters=5).fit(df)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    centroid[cut] = centroids\n",
    "    \n",
    "\n",
    "    plt.scatter(df['x'], df['y'], c= kmeans.labels_.astype(float), s=20, alpha=0.5)\n",
    "    plt.scatter(centroids.flatten()[::2],centroids.flatten()[1::2],s=50,c='r')\n",
    "    plt.xlim(0.2,3.5)\n",
    "    plt.ylim(0,2000)\n",
    "    plt.xlabel('depth (µm)')\n",
    "    plt.ylabel('size (px\\u00B2)')\n",
    "    plt.title(cut)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "non linear fit by a power law a+b*x^c\n",
    "'''\n",
    "\n",
    "# 3rd party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def f(x,a,b,c): # Power law fit\n",
    "    return a + b*x**(c)\n",
    "\n",
    "color = {\"cut 117\":'k',\"cut 119\":'r',\"cut 121\":'b',\"cut 134\":'c',\"cut 139\":'y'}\n",
    "xt = []\n",
    "yt = []\n",
    "for x,y in centroid.items():\n",
    "    plt.scatter(y.flatten()[::2],y.flatten()[1::2],s=50,label=x, c=color[x])\n",
    "    xt.extend(y.flatten()[::2])\n",
    "    yt.extend(y.flatten()[1::2])\n",
    "\n",
    "plt.xlabel('depth (µm)')\n",
    "plt.ylabel('size (px\\u00B2)')   \n",
    "plt.legend()\n",
    "par,cov = scipy.optimize.curve_fit(f,xt,yt)\n",
    "x_fit = np.arange(0.4,2,0.01)\n",
    "_ =plt.plot(x_fit,f(x_fit,*par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
